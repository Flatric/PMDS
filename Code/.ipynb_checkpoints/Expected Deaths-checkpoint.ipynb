{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d8b565-48c3-41ed-8a79-e7b283763e0a",
   "metadata": {},
   "source": [
    "# Excess Mortality Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440ed96-3426-4908-843f-e42f3665367e",
   "metadata": {},
   "source": [
    "In this notebook we will attempt to calculate the excess deaths in Germany during the COVID-19 epidemic. We base our analysis on the dataset of weekly deaths since 2000 in Germany obtained from eurostat. Based on the trend and periodogram of the time series we generate numerical features that capture the temporal structure of the deaths in Germany enabling us to train and compare various machine learning algorithms that can then be used to predict the mortality of the time period starting from 2020 in the absence of corona. \n",
    "This prediction can then be compared to the actual mortality yielding an estimation for the number of excess deaths during the COVID-19 pandemic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e0214-f671-40e5-b9e1-dc16d295e1d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Data and relevant Libraries\n",
    "We conduct our analysis using Python. The main libraries used are`scipy` which we use to analyze the seasonality of the weekly deaths, `statsmodels` for generating the temporal features used in our machine learning pipeline which itself is based `scikit-learn` and `matplotlib` for visualizing our results. Besides that we also use `pandas` and `numpy` to perform some auxillary tasks.\n",
    "\n",
    "The data comes from [EuroStat](https://ec.europa.eu/eurostat/data/database) as an excel table containing the weekly Deaths in Germany from 2000 until now (March 2022). The preprocessing of this data is done in the `Analytics` module by the function `eurostats_loader` which returns the data as a pandas DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcc7955-d332-429e-b211-a20aa0be5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Analytics import eurostats_loader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
    "from scipy.signal import periodogram, detrend\n",
    "from sklearn.linear_model  import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adjust color cycle of plots (first line HS-blue second line burgundy red, turqouise)\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#0645ad\", \"#800020\", \"#1EB9E7\", ]) \n",
    "mpl.rcParams['figure.dpi'] = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7978551-653c-41a9-9706-b709f93cc1ba",
   "metadata": {},
   "source": [
    "## 2. Preparing the Data.\n",
    "We discard all observations before 2008 since including them would distort the trend estimation for our model due to the fact that the demography or Germany is undergoing significant changes, since 2008 the mortality has been increasing linerarly specifically. \n",
    "\n",
    "The preprocessed data is then split into a training-, validation- and test-set. We use a validation set for comparing model performace. The test set contains the periods from 2020 onward, hence the time period since COVID-19 started to spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "182c3a06-63de-4b25-8c7f-35b15a788beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weekly_Deaths = eurostats_loader()\n",
    "Weekly_Deaths = Weekly_Deaths[Weekly_Deaths.index.year >= 2008]\n",
    "\n",
    "Weekly_Deaths_train = Weekly_Deaths[Weekly_Deaths.index.year < 2019]\n",
    "Weekly_Deaths_val = Weekly_Deaths[Weekly_Deaths.index.year == 2019]\n",
    "Weekly_Deaths_test = Weekly_Deaths[Weekly_Deaths.index.year > 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ea6d62-682b-4589-9056-3c1c7d8474c0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Weekly Deaths in Germany 2000-2022')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weekly_Deaths.Value.plot().set_title(\"Weekly Deaths in Germany 2000-2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe06a6d-3569-4977-9ffb-bbcea296377f",
   "metadata": {},
   "source": [
    "## 3. Analyzing seasonality\n",
    "A visual inspectation of `Weekly_Deaths` gives evidence for the existence of a seasonal pattern in the mortality. In general seasonal patterns can be modeled via sine and cosine periodic functions. The mathematical theory of Fourier Analysis provides tools for finding the periodic functions that best model the seasonality of our data. The `periodogram` function from scipy is one such tool. By mapping the time series from its representation in the time domain (x-axis) to a domain (x-axis) of frequencies where peaks indicate that a sine or cosine function with the corresponding frequency models a part or the entire seasonality of the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99df453-c169-41ab-8aab-e6e0adc46da4",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/72/Fourier_transform_time_and_frequency_domains_%28small%29.gif\"/>\n",
    "<figcaption style=\"color:white;font-size:10px;\" face=\"Droid Sans\">Visual Intuition behind the Periodogram. </figcaption>   \n",
    "</center>\n",
    "<figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5573a-5bd9-468d-9466-3a0de02a8a98",
   "metadata": {},
   "source": [
    "In addition to the time series to be mapped, we can set the frequency domain to which it will be mapped via the argument `fs`. By setting fs to be equal to the number of weeks per year scale the frequency domain such that one year is equal to a frequency of one. For the periodogram to be accurate the time series needs to be stationary which can be achieved by detrending it beforehand. The type of trend can be set via the `detrend` argument, to us a linear trend seems to be an appropriate choice. Setting the argument `scaling` to `\"spectrum\"` makes sure that the following equality is fulfilled:\n",
    "\n",
    "$$ \\operatorname{Var}(X)=\\int_{-\\infty}^{\\infty} S_{x x}(f) d f \\hspace{1cm}(3.1)$$\n",
    "\n",
    "where $X$ are the values of the time series and $f$ the frequencies returned by `periodogram`. The function $S_{x x}(f)$ is the so-called power spectrum which due to the above equation can be interpreted as the amount of variance of the times series explained by each frequency $f$. Note that due to the fact that $f$ returned by the periodogram is discrete the integration turns into summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822af1d0-205f-4d33-ac39-d88f8bb726f9",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1W\")\n",
    "print(f\"fs: {fs}\\n\")\n",
    "f, Sxx = periodogram(Weekly_Deaths_train.Value, detrend=\"linear\", fs=fs, scaling=\"spectrum\")\n",
    "# The Power Spectrum Sxx indicates the share of each frequency of the variance of the Time Series.\n",
    "print(\"Is equation (3.1) fulfilled?\\n\")\n",
    "print(f\"Sxx: {np.sum(Sxx)}\")\n",
    "# Weekly Deaths need to be detrended because the periodogram is fitted on the detrended series.\n",
    "print(f\"Var: {np.var(detrend(Weekly_Deaths_train.Value))}\\n\")\n",
    "# Peak of periodogram\n",
    "print(f\"Peak frequency: {f[np.argmax(Sxx)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef43b7c-c372-47f6-a955-1968ad272fbc",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(f, Sxx)\n",
    "plt.xlabel('Frequency [Year]')\n",
    "plt.ylabel('Power Spectrum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99204665-3e72-4189-85d2-ec51aad07f9a",
   "metadata": {},
   "source": [
    "## 4. Generating temporal Features\n",
    "In this section we create features for our machine learning models. Although it would be feasible to take the entire time axis of the time series `Weekly Deaths` as feature vector this would force us to use a highly complex model because it would need to deduce seasonality and trend together from time dates alone. Due to the fact that the time series appears to have a trend and periodogram analysis in section 3 gave overwhelming evidence for yearly seasonality we decided to directly handcraft these features since this enhances the explainability of our models which is crucial in a medical context.\n",
    "\n",
    "The function `DeterministicProcess` from statsmodels is used to generate features for each component of our time series. DeterministicProcess will generate features for the three components we use to model our time series, namely the average of the series or its *level*, its trend and its seasonality. Although DeterministicProcess has to arguments `seasonal` and `fourier` we decide to create the periodic features ourselves via `CalendarFourier` since it enables us to control the frequency of the functions. After comparing model performance for different values of `order` in CaldendarFourier we decided to set it to 3. This gives the model enough flexibility to adapt to the not entirerly sinosoidal shape of the peaks of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa52179-6ad0-40f7-9d4b-df0a25ac5816",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fourierA = CalendarFourier(freq=\"A\", order=3)\n",
    "\n",
    "\n",
    "y_train = Weekly_Deaths_train.Value\n",
    "\n",
    "dp = DeterministicProcess(index=y_train.index.to_period(\"W\"),\n",
    "                          constant=True,               # dummy feature for bias (y-intercept)\n",
    "                          order=1,                     # trend (order 1 means linear)\n",
    "                          additional_terms=[fourierA],) # annual seasonality (fourier)\n",
    "\n",
    "\n",
    "X_train = dp.in_sample()\n",
    "\n",
    "val_index = Weekly_Deaths_val.index\n",
    "X_val = dp.out_of_sample(len(val_index), val_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073cf53-b3b3-46a3-a660-51b435e8d7e2",
   "metadata": {},
   "source": [
    "For a more closer look at the features generated by `dp.in_sample()` run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ca31b-66ed-4dd8-8d0b-a1b68e56e675",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c97cb-0164-4d4b-b050-27887c6cbeb4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train[[\"sin(1,freq=A-DEC)\", \"sin(2,freq=A-DEC)\", \"sin(3,freq=A-DEC)\"]].loc[X_train.index<\"2009\"].plot().set_title(\"Sine Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1795a-d799-4998-9e7d-68af596928bd",
   "metadata": {},
   "source": [
    "## 5. Fit Models\n",
    "We fit and compare the performance of two models on the training data `D = (X_train, y_train)` containing the time series components as features and the Weekly Deaths until 2020 as predicted variable. In essence the models try to combine the components generated by `DeterministicProcess` in such a way that their sum looks like `y_train`. While Linear Regression is a well known technique the second algorithm we used might not be familiar to everyone. We also had to decide which loss function to use to measure the *goodness of fit* of our models and make asses the trade-off between performance and interpretability.\n",
    "\n",
    "### 5.1 Gradient Boosting\n",
    "The idea behind Gradient Boosting is the stepwise combination of so called weak learners into an ensemble of models whose aggregate predictions are comparable to those of a more complex model. In our case the weak learners are *decision stumps* meaning decision trees with only one node. Gradient boosting works by calculating the gradient of the loss function after adding a new decision stump to the ensemble and then fitting a new decision stump to it, this procedure is repeated `n_estimator` of times. It can be thought of as a form of regularized gradient descent which prevents the model from overfitting by simply matching the data exactly since each decision stump is trained on the entire training data `D`. \n",
    "\n",
    "### 5.2 Evaluating the Models\n",
    "\n",
    "We chose the *mean squared error* (mse) as loss function to measure the goodness of fit of our models. One advantage of the mse is that it penalizes large deviations from the true value while being lenient on small deviations. In the context of excess mortality prediction this makes sense since it isn't to important to get the number of deaths exactly right but rather to approximate them sufficiently well. For better interpretability of the performance we judge the model performance by the absolute error instead of the mse. A model that has a lower absolute error also has a lower mse. In the plot below it can be seen that `GradientBoostingRegressor` has roughly 10% lower absolute error than `LinearRegression`. It would be of interest to find a way to measure the statistical significance of such a performance improvement. It would also be interesting to try to come up with an evaluation strategy that uses more than one validation set, unfortunately the temporal structure of or data prevents us from using cross-validation for more accurate performance assesment.\n",
    "\n",
    "## 5.3. Which Model to choose?\n",
    "In the end we have to decide which model to use not only based on its performance but also based on its interpretability. Although `LinearRegression` is not alot worse than `GradientBoostingRegressor` and has a far superior interpretability, we have yet to find a scientific article that used `GradientBoostingRegressor` for excess mortality prediction. For the sake of novelty we therefore chose to calculate the excess mortality for 2020-2022 via our Gradient Boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77332455-39cb-4532-8c2c-cb714466d4da",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Models = [LinearRegression(fit_intercept=False),\n",
    "          GradientBoostingRegressor(max_depth=1, n_estimators=150)]\n",
    "\n",
    "num_models = len(Models)\n",
    "fig, axes = plt.subplots(num_models,2,layout=\"constrained\",sharex=\"col\",figsize = (15,8))\n",
    "\n",
    "for i,model in enumerate(Models):\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = pd.Series(model.predict(X_train).reshape(-1), index=y_train.index)\n",
    "    y_val = pd.Series(model.predict(X_val).reshape(-1), index=val_index)\n",
    "    abs_err = np.mean(abs(y_val-Weekly_Deaths_val.Value))\n",
    "\n",
    "    #plot parameter\n",
    "    ax_train = axes[i,0]\n",
    "    y_pred.plot(ax=ax_train)\n",
    "    Weekly_Deaths_train.Value.plot(ax=ax_train)\n",
    "    ax_train.fill_between(y_pred.index,y1= Weekly_Deaths_train.Value,y2 = y_pred.values, color= 'lightblue', alpha=0.4)\n",
    "\n",
    "    ax_val = axes[i,1]\n",
    "    ax_val.axes.annotate(f\"Abs_err: {abs_err:.2f}\", xy=(0.8,0.9), xycoords='axes fraction', fontsize=10)\n",
    "    y_val.plot(ax=ax_val)\n",
    "    Weekly_Deaths_val.Value.plot(ax=ax_val)\n",
    "\n",
    "    ax_val.fill_between(y_val.index,y1= Weekly_Deaths_val.Value,y2 = y_val.values, color= 'lightblue', alpha=0.4)\n",
    "    ax_train.set_title(type(model).__name__, y=1.05,x=1.1, pad=5, size=14,fontweight=\"bold\")\n",
    "    ax_train.grid(linewidth=0.5)\n",
    "    ax_val.grid(linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85033d71",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Weekly_Deaths_val.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39eced-1ede-44cd-ab5a-cb2ff16a47c8",
   "metadata": {},
   "source": [
    "## 6. Excess Mortality Estimation\n",
    "We now use our Gradient Boosting model to estimate the number of excess deaths between the start of 2020 and March 2022 in Germany. Our model estimates roughly $131000$ excess deaths in comparisson to two \"normal\" years with winterly influenza waves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859bfbf-3fe4-4ee4-bc0b-a138ab338bdc",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_comb_train = Weekly_Deaths[Weekly_Deaths.index.year < 2020]\n",
    "\n",
    "dp = DeterministicProcess(index=y_comb_train.index.to_period(\"W\"),\n",
    "                          constant=True,               # dummy feature for bias (y-intercept)\n",
    "                          order=1,                     # trend (order 1 means linear)\n",
    "                          additional_terms=[fourierA],)  # annual seasonality (fourier)\n",
    "\n",
    "\n",
    "X_comb_train = dp.in_sample()\n",
    "\n",
    "model = GradientBoostingRegressor(max_depth=1)\n",
    "model.fit(X_comb_train, y_comb_train)\n",
    "y_comb_pred = pd.Series(model.predict(X_comb_train).reshape(-1), index=y_comb_train.index)\n",
    "\n",
    "\n",
    "test_index = Weekly_Deaths_test.index\n",
    "X_test = dp.out_of_sample(len(test_index), test_index)\n",
    "y_test = pd.Series(model.predict(X_test).reshape(-1), index=test_index)\n",
    "excess_deaths = np.sum(Weekly_Deaths_test.Value-y_test)\n",
    "fig_test, ax_test = plt.subplots()\n",
    "ax_test.axes.annotate(f\"Estimated Excess Deaths: {excess_deaths:.0f}\", xy=(0.55,0.9), xycoords='axes fraction', fontsize=8)\n",
    "y_test.plot(ax=ax_test)\n",
    "Weekly_Deaths_test.Value.plot(ax=ax_test).set_title(\"Estimated Excess Deaths in Germany 2020-2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf62c06-15a8-4734-bde9-d850dc21d175",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
